{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN Generating Fake Faces(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u439-NbNQtUVDdlPtljAamN3WxAYBxAD",
      "authorship_tag": "ABX9TyMScdQdSql/iMbwxrH7gKep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biditdas18/Generative_Adversarial_Network_Fake_Celebrity_Faces/blob/master/GAN_Generating_Fake_Faces(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UBgCX7ttRHo",
        "colab_type": "text"
      },
      "source": [
        "This section of code also deals with the development of GAN and achieves same objective as that of the file GAN Generating Fake Faces but with better visualization, code simplification and plotting of generator and discriminator loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frT_nRDPqI54",
        "colab_type": "code",
        "outputId": "af0c85b0-4943-4974-c4b5-d2ef119aa107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/Kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9dmhIyqkR0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make necessary imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense, Reshape, Flatten,\\\n",
        "Conv2D,Conv2DTranspose, LeakyReLU, Dropout\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSxSCjlHkZUz",
        "colab_type": "code",
        "outputId": "2d72565c-0bcb-4d7b-9ac3-22383274cc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load in the data\n",
        "def load_real_samples():\n",
        "  # load the face dataset\n",
        "  data = np.load('img_align_celeba.npz')\n",
        "  X = data['arr_0']\n",
        "  # Convert from unsigned int to float\n",
        "  X = X.astype('float32')\n",
        "  # scale from [0,255] to [-1,1]\n",
        "  X = (X - 127.5)/127.5\n",
        "  return X\n",
        "\n",
        "x_train = load_real_samples()\n",
        "print(\"x_train.shape:\", x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape: (50000, 80, 80, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch_DERhWpiBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the data\n",
        "N, H, W, C = x_train.shape\n",
        "D = H * W * C\n",
        "# x_train = x_train.reshape(-1, D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3FwOYpVmvvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensionality of the latent space\n",
        "latent_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAP-gponmzBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the generator model\n",
        "def build_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  # foundation of 5 x 5 feature map\n",
        "  n_nodes = 128 * 5 * 5\n",
        "  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((5,5,128)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(128,(4,4),strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(3,(5,5),activation='tanh', padding='same'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgE778o1nH0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the discriminator model\n",
        "def build_discriminator(img_size=(80,80,3)):\n",
        "\tmodel = Sequential()\n",
        "\t\n",
        "\tmodel.add(Conv2D(128, (5,5), padding='same', input_shape=img_size))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\n",
        "\tmodel.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\n",
        "\tmodel.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\n",
        "\tmodel.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\n",
        "\tmodel.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# classifier\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDzRTyh7rdxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile both models in preparation for training\n",
        "\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(img_size=(80,80,3))\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.0002, 0.5),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Build and compile the combined model\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Create an input to represent noise sample from latent space\n",
        "z = Input(shape=(latent_dim,))\n",
        "\n",
        "# Pass noise through generator to get an image\n",
        "img = generator(z)\n",
        "\n",
        "# Make sure only the generator is trained\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The true output is fake, but we label them real!\n",
        "fake_pred = discriminator(img)\n",
        "\n",
        "# Create the combined model object\n",
        "combined_model = Model(z, fake_pred)\n",
        "\n",
        "# Compile the combined model\n",
        "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL7Ic9PyrlrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the GAN\n",
        "\n",
        "\n",
        "# Config\n",
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_period = 200 # every `sample_period` steps generate and save some data\n",
        "\n",
        "\n",
        "# Create batch labels to use when calling train_on_batch\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "# Store the losses\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Create a folder to store generated images\n",
        "if not os.path.exists('gan_images1'):\n",
        "  os.makedirs('gan_images1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0bQ7jWtsxaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to generate a grid of random samples from the generator\n",
        "# and save them to a file\n",
        "def sample_images(epoch):\n",
        "  rows, cols = 5, 5\n",
        "  noise = np.random.randn(rows * cols, latent_dim)\n",
        "  imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  imgs = 0.5 * imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(rows, cols)\n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "    for j in range(cols):\n",
        "      axs[i,j].imshow(imgs[idx])\n",
        "      axs[i,j].axis('off')\n",
        "      idx += 1\n",
        "  fig.savefig(\"gan_images1/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-oMf5qrsz8F",
        "colab_type": "code",
        "outputId": "4bcbbe0b-acc8-4ec3-beb6-e9079fc7d7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "  ###########################\n",
        "  ### Train discriminator ###\n",
        "  ###########################\n",
        "  \n",
        "  # Select a random batch of images\n",
        "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "  real_imgs = x_train[idx]\n",
        "  \n",
        "  # Generate fake images\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  fake_imgs = generator.predict(noise)\n",
        "  \n",
        "  # Train the discriminator\n",
        "  # both loss and accuracy are returned\n",
        "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
        "  \n",
        "  \n",
        "  #######################\n",
        "  ### Train generator ###\n",
        "  #######################\n",
        "  \n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "  \n",
        "  # do it again!\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "  \n",
        "  # Save the losses\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "  \n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n",
        "      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
        "  \n",
        "  if epoch % sample_period == 0:\n",
        "    sample_images(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/30000, d_loss: 0.69,       d_acc: 0.33, g_loss: 0.69\n",
            "epoch: 101/30000, d_loss: 0.15,       d_acc: 0.97, g_loss: 2.81\n",
            "epoch: 201/30000, d_loss: 0.38,       d_acc: 0.88, g_loss: 1.11\n",
            "epoch: 301/30000, d_loss: 0.43,       d_acc: 0.88, g_loss: 2.36\n",
            "epoch: 401/30000, d_loss: 0.40,       d_acc: 0.80, g_loss: 2.15\n",
            "epoch: 501/30000, d_loss: 0.36,       d_acc: 0.88, g_loss: 2.16\n",
            "epoch: 601/30000, d_loss: 0.47,       d_acc: 0.80, g_loss: 2.05\n",
            "epoch: 701/30000, d_loss: 0.39,       d_acc: 0.84, g_loss: 1.75\n",
            "epoch: 801/30000, d_loss: 0.42,       d_acc: 0.81, g_loss: 2.37\n",
            "epoch: 901/30000, d_loss: 0.45,       d_acc: 0.75, g_loss: 2.00\n",
            "epoch: 1001/30000, d_loss: 0.57,       d_acc: 0.72, g_loss: 1.66\n",
            "epoch: 1101/30000, d_loss: 0.39,       d_acc: 0.86, g_loss: 2.19\n",
            "epoch: 1201/30000, d_loss: 0.44,       d_acc: 0.86, g_loss: 1.81\n",
            "epoch: 1301/30000, d_loss: 0.57,       d_acc: 0.72, g_loss: 1.72\n",
            "epoch: 1401/30000, d_loss: 0.46,       d_acc: 0.78, g_loss: 2.03\n",
            "epoch: 1501/30000, d_loss: 0.43,       d_acc: 0.83, g_loss: 1.94\n",
            "epoch: 1601/30000, d_loss: 0.27,       d_acc: 0.97, g_loss: 1.70\n",
            "epoch: 1701/30000, d_loss: 0.64,       d_acc: 0.67, g_loss: 1.41\n",
            "epoch: 1801/30000, d_loss: 0.44,       d_acc: 0.77, g_loss: 1.87\n",
            "epoch: 1901/30000, d_loss: 0.48,       d_acc: 0.77, g_loss: 1.66\n",
            "epoch: 2001/30000, d_loss: 0.42,       d_acc: 0.81, g_loss: 1.48\n",
            "epoch: 2101/30000, d_loss: 0.47,       d_acc: 0.78, g_loss: 1.48\n",
            "epoch: 2201/30000, d_loss: 0.48,       d_acc: 0.78, g_loss: 1.54\n",
            "epoch: 2301/30000, d_loss: 0.54,       d_acc: 0.72, g_loss: 1.58\n",
            "epoch: 2401/30000, d_loss: 0.52,       d_acc: 0.75, g_loss: 1.81\n",
            "epoch: 2501/30000, d_loss: 0.43,       d_acc: 0.81, g_loss: 1.42\n",
            "epoch: 2601/30000, d_loss: 0.48,       d_acc: 0.80, g_loss: 1.34\n",
            "epoch: 2701/30000, d_loss: 0.75,       d_acc: 0.55, g_loss: 1.40\n",
            "epoch: 2801/30000, d_loss: 0.57,       d_acc: 0.69, g_loss: 1.22\n",
            "epoch: 2901/30000, d_loss: 0.50,       d_acc: 0.75, g_loss: 1.49\n",
            "epoch: 3001/30000, d_loss: 0.47,       d_acc: 0.81, g_loss: 1.26\n",
            "epoch: 3101/30000, d_loss: 0.61,       d_acc: 0.64, g_loss: 1.48\n",
            "epoch: 3201/30000, d_loss: 0.52,       d_acc: 0.77, g_loss: 1.25\n",
            "epoch: 3301/30000, d_loss: 0.61,       d_acc: 0.70, g_loss: 1.25\n",
            "epoch: 3401/30000, d_loss: 0.60,       d_acc: 0.66, g_loss: 1.38\n",
            "epoch: 3501/30000, d_loss: 0.55,       d_acc: 0.77, g_loss: 1.27\n",
            "epoch: 3601/30000, d_loss: 0.61,       d_acc: 0.70, g_loss: 1.19\n",
            "epoch: 3701/30000, d_loss: 0.60,       d_acc: 0.69, g_loss: 1.26\n",
            "epoch: 3801/30000, d_loss: 0.57,       d_acc: 0.75, g_loss: 1.18\n",
            "epoch: 3901/30000, d_loss: 0.56,       d_acc: 0.77, g_loss: 1.35\n",
            "epoch: 4001/30000, d_loss: 0.64,       d_acc: 0.66, g_loss: 1.11\n",
            "epoch: 4101/30000, d_loss: 0.55,       d_acc: 0.70, g_loss: 1.15\n",
            "epoch: 4201/30000, d_loss: 0.61,       d_acc: 0.61, g_loss: 1.31\n",
            "epoch: 4301/30000, d_loss: 0.54,       d_acc: 0.70, g_loss: 1.36\n",
            "epoch: 4401/30000, d_loss: 0.67,       d_acc: 0.64, g_loss: 1.25\n",
            "epoch: 4501/30000, d_loss: 0.54,       d_acc: 0.72, g_loss: 1.17\n",
            "epoch: 4601/30000, d_loss: 0.54,       d_acc: 0.77, g_loss: 1.26\n",
            "epoch: 4701/30000, d_loss: 0.57,       d_acc: 0.73, g_loss: 1.09\n",
            "epoch: 4801/30000, d_loss: 0.66,       d_acc: 0.59, g_loss: 1.02\n",
            "epoch: 4901/30000, d_loss: 0.56,       d_acc: 0.70, g_loss: 1.02\n",
            "epoch: 5001/30000, d_loss: 0.51,       d_acc: 0.78, g_loss: 1.21\n",
            "epoch: 5101/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 1.14\n",
            "epoch: 5201/30000, d_loss: 0.70,       d_acc: 0.56, g_loss: 1.14\n",
            "epoch: 5301/30000, d_loss: 0.61,       d_acc: 0.70, g_loss: 1.07\n",
            "epoch: 5401/30000, d_loss: 0.65,       d_acc: 0.56, g_loss: 1.23\n",
            "epoch: 5501/30000, d_loss: 0.71,       d_acc: 0.59, g_loss: 1.17\n",
            "epoch: 5601/30000, d_loss: 0.67,       d_acc: 0.55, g_loss: 1.07\n",
            "epoch: 5701/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 1.02\n",
            "epoch: 5801/30000, d_loss: 0.50,       d_acc: 0.86, g_loss: 1.18\n",
            "epoch: 5901/30000, d_loss: 0.56,       d_acc: 0.72, g_loss: 1.08\n",
            "epoch: 6001/30000, d_loss: 0.61,       d_acc: 0.69, g_loss: 1.01\n",
            "epoch: 6101/30000, d_loss: 0.65,       d_acc: 0.58, g_loss: 1.19\n",
            "epoch: 6201/30000, d_loss: 0.57,       d_acc: 0.67, g_loss: 1.09\n",
            "epoch: 6301/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 1.09\n",
            "epoch: 6401/30000, d_loss: 0.58,       d_acc: 0.69, g_loss: 0.93\n",
            "epoch: 6501/30000, d_loss: 0.62,       d_acc: 0.69, g_loss: 1.00\n",
            "epoch: 6601/30000, d_loss: 0.52,       d_acc: 0.78, g_loss: 1.05\n",
            "epoch: 6701/30000, d_loss: 0.62,       d_acc: 0.61, g_loss: 1.10\n",
            "epoch: 6801/30000, d_loss: 0.53,       d_acc: 0.80, g_loss: 0.97\n",
            "epoch: 6901/30000, d_loss: 0.62,       d_acc: 0.62, g_loss: 1.00\n",
            "epoch: 7001/30000, d_loss: 0.62,       d_acc: 0.67, g_loss: 1.14\n",
            "epoch: 7101/30000, d_loss: 0.60,       d_acc: 0.62, g_loss: 1.29\n",
            "epoch: 7201/30000, d_loss: 0.59,       d_acc: 0.69, g_loss: 1.00\n",
            "epoch: 7301/30000, d_loss: 0.52,       d_acc: 0.80, g_loss: 1.15\n",
            "epoch: 7401/30000, d_loss: 0.51,       d_acc: 0.81, g_loss: 1.11\n",
            "epoch: 7501/30000, d_loss: 0.58,       d_acc: 0.72, g_loss: 1.04\n",
            "epoch: 7601/30000, d_loss: 0.55,       d_acc: 0.75, g_loss: 1.27\n",
            "epoch: 7701/30000, d_loss: 0.62,       d_acc: 0.67, g_loss: 1.23\n",
            "epoch: 7801/30000, d_loss: 0.57,       d_acc: 0.73, g_loss: 0.95\n",
            "epoch: 7901/30000, d_loss: 0.55,       d_acc: 0.75, g_loss: 1.25\n",
            "epoch: 8001/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 92.38\n",
            "epoch: 8101/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 122.06\n",
            "epoch: 8201/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 163.85\n",
            "epoch: 8301/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 151.11\n",
            "epoch: 8401/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 132.89\n",
            "epoch: 8501/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 140.94\n",
            "epoch: 8601/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 109.03\n",
            "epoch: 8701/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 198.41\n",
            "epoch: 8801/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 58.55\n",
            "epoch: 8901/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 62.69\n",
            "epoch: 9001/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 65.19\n",
            "epoch: 9101/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 59.51\n",
            "epoch: 9201/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 59.79\n",
            "epoch: 9301/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 65.22\n",
            "epoch: 9401/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 65.66\n",
            "epoch: 9501/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 66.68\n",
            "epoch: 9601/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 62.01\n",
            "epoch: 9701/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 61.00\n",
            "epoch: 9801/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 63.19\n",
            "epoch: 9901/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 65.40\n",
            "epoch: 10001/30000, d_loss: 0.08,       d_acc: 0.98, g_loss: 22.09\n",
            "epoch: 10101/30000, d_loss: 1.56,       d_acc: 0.91, g_loss: 53.05\n",
            "epoch: 10201/30000, d_loss: 0.33,       d_acc: 0.84, g_loss: 3.90\n",
            "epoch: 10301/30000, d_loss: 0.46,       d_acc: 0.72, g_loss: 2.34\n",
            "epoch: 10401/30000, d_loss: 0.53,       d_acc: 0.78, g_loss: 2.10\n",
            "epoch: 10501/30000, d_loss: 0.48,       d_acc: 0.78, g_loss: 2.05\n",
            "epoch: 10601/30000, d_loss: 0.51,       d_acc: 0.83, g_loss: 2.38\n",
            "epoch: 10701/30000, d_loss: 0.43,       d_acc: 0.73, g_loss: 2.09\n",
            "epoch: 10801/30000, d_loss: 0.31,       d_acc: 0.92, g_loss: 1.93\n",
            "epoch: 10901/30000, d_loss: 0.31,       d_acc: 0.92, g_loss: 1.80\n",
            "epoch: 11001/30000, d_loss: 0.54,       d_acc: 0.72, g_loss: 1.77\n",
            "epoch: 11101/30000, d_loss: 0.44,       d_acc: 0.80, g_loss: 2.03\n",
            "epoch: 11201/30000, d_loss: 0.39,       d_acc: 0.78, g_loss: 1.96\n",
            "epoch: 11301/30000, d_loss: 0.46,       d_acc: 0.75, g_loss: 1.71\n",
            "epoch: 11401/30000, d_loss: 0.49,       d_acc: 0.73, g_loss: 2.31\n",
            "epoch: 11501/30000, d_loss: 0.34,       d_acc: 0.88, g_loss: 2.24\n",
            "epoch: 11601/30000, d_loss: 0.27,       d_acc: 0.91, g_loss: 2.21\n",
            "epoch: 11701/30000, d_loss: 0.38,       d_acc: 0.81, g_loss: 2.07\n",
            "epoch: 11801/30000, d_loss: 0.27,       d_acc: 0.88, g_loss: 1.90\n",
            "epoch: 11901/30000, d_loss: 0.38,       d_acc: 0.88, g_loss: 1.82\n",
            "epoch: 12001/30000, d_loss: 0.45,       d_acc: 0.77, g_loss: 1.66\n",
            "epoch: 12101/30000, d_loss: 0.47,       d_acc: 0.81, g_loss: 1.49\n",
            "epoch: 12201/30000, d_loss: 0.38,       d_acc: 0.83, g_loss: 2.10\n",
            "epoch: 12301/30000, d_loss: 0.50,       d_acc: 0.75, g_loss: 1.48\n",
            "epoch: 12401/30000, d_loss: 0.31,       d_acc: 0.91, g_loss: 2.06\n",
            "epoch: 12501/30000, d_loss: 0.41,       d_acc: 0.84, g_loss: 1.95\n",
            "epoch: 12601/30000, d_loss: 0.51,       d_acc: 0.75, g_loss: 1.44\n",
            "epoch: 12701/30000, d_loss: 0.38,       d_acc: 0.88, g_loss: 1.86\n",
            "epoch: 12801/30000, d_loss: 0.44,       d_acc: 0.80, g_loss: 1.66\n",
            "epoch: 12901/30000, d_loss: 0.55,       d_acc: 0.73, g_loss: 1.81\n",
            "epoch: 13001/30000, d_loss: 0.48,       d_acc: 0.80, g_loss: 1.33\n",
            "epoch: 13101/30000, d_loss: 0.49,       d_acc: 0.81, g_loss: 1.58\n",
            "epoch: 13201/30000, d_loss: 0.65,       d_acc: 0.58, g_loss: 1.58\n",
            "epoch: 13301/30000, d_loss: 0.51,       d_acc: 0.78, g_loss: 1.22\n",
            "epoch: 13401/30000, d_loss: 0.58,       d_acc: 0.66, g_loss: 1.23\n",
            "epoch: 13501/30000, d_loss: 0.58,       d_acc: 0.72, g_loss: 1.61\n",
            "epoch: 13601/30000, d_loss: 0.51,       d_acc: 0.75, g_loss: 1.34\n",
            "epoch: 13701/30000, d_loss: 0.48,       d_acc: 0.80, g_loss: 1.46\n",
            "epoch: 13801/30000, d_loss: 0.45,       d_acc: 0.81, g_loss: 1.25\n",
            "epoch: 13901/30000, d_loss: 0.49,       d_acc: 0.72, g_loss: 1.28\n",
            "epoch: 14001/30000, d_loss: 0.50,       d_acc: 0.73, g_loss: 1.86\n",
            "epoch: 14101/30000, d_loss: 0.49,       d_acc: 0.70, g_loss: 1.52\n",
            "epoch: 14201/30000, d_loss: 0.54,       d_acc: 0.80, g_loss: 1.72\n",
            "epoch: 14301/30000, d_loss: 0.39,       d_acc: 0.84, g_loss: 1.57\n",
            "epoch: 14401/30000, d_loss: 0.47,       d_acc: 0.77, g_loss: 1.65\n",
            "epoch: 14501/30000, d_loss: 0.53,       d_acc: 0.69, g_loss: 1.68\n",
            "epoch: 14601/30000, d_loss: 0.37,       d_acc: 0.84, g_loss: 1.90\n",
            "epoch: 14701/30000, d_loss: 0.51,       d_acc: 0.77, g_loss: 1.70\n",
            "epoch: 14801/30000, d_loss: 0.51,       d_acc: 0.78, g_loss: 1.37\n",
            "epoch: 14901/30000, d_loss: 0.45,       d_acc: 0.81, g_loss: 1.53\n",
            "epoch: 15001/30000, d_loss: 0.55,       d_acc: 0.72, g_loss: 1.68\n",
            "epoch: 15101/30000, d_loss: 0.34,       d_acc: 0.91, g_loss: 1.90\n",
            "epoch: 15201/30000, d_loss: 0.62,       d_acc: 0.62, g_loss: 1.63\n",
            "epoch: 15301/30000, d_loss: 0.48,       d_acc: 0.75, g_loss: 2.10\n",
            "epoch: 15401/30000, d_loss: 0.41,       d_acc: 0.83, g_loss: 2.01\n",
            "epoch: 15501/30000, d_loss: 0.41,       d_acc: 0.81, g_loss: 2.46\n",
            "epoch: 15601/30000, d_loss: 0.40,       d_acc: 0.84, g_loss: 2.05\n",
            "epoch: 15701/30000, d_loss: 0.43,       d_acc: 0.75, g_loss: 1.95\n",
            "epoch: 15801/30000, d_loss: 0.40,       d_acc: 0.83, g_loss: 1.45\n",
            "epoch: 15901/30000, d_loss: 0.43,       d_acc: 0.83, g_loss: 1.77\n",
            "epoch: 16001/30000, d_loss: 0.47,       d_acc: 0.81, g_loss: 2.20\n",
            "epoch: 16101/30000, d_loss: 0.61,       d_acc: 0.69, g_loss: 2.22\n",
            "epoch: 16201/30000, d_loss: 0.46,       d_acc: 0.80, g_loss: 1.50\n",
            "epoch: 16301/30000, d_loss: 0.46,       d_acc: 0.78, g_loss: 1.89\n",
            "epoch: 16401/30000, d_loss: 0.43,       d_acc: 0.84, g_loss: 1.57\n",
            "epoch: 16501/30000, d_loss: 0.49,       d_acc: 0.83, g_loss: 1.68\n",
            "epoch: 16601/30000, d_loss: 0.48,       d_acc: 0.78, g_loss: 1.90\n",
            "epoch: 16701/30000, d_loss: 0.44,       d_acc: 0.78, g_loss: 1.91\n",
            "epoch: 16801/30000, d_loss: 0.37,       d_acc: 0.88, g_loss: 2.11\n",
            "epoch: 16901/30000, d_loss: 0.39,       d_acc: 0.83, g_loss: 1.77\n",
            "epoch: 17001/30000, d_loss: 0.42,       d_acc: 0.84, g_loss: 1.51\n",
            "epoch: 17101/30000, d_loss: 0.49,       d_acc: 0.75, g_loss: 1.71\n",
            "epoch: 17201/30000, d_loss: 0.30,       d_acc: 0.88, g_loss: 2.07\n",
            "epoch: 17301/30000, d_loss: 0.50,       d_acc: 0.81, g_loss: 2.70\n",
            "epoch: 17401/30000, d_loss: 0.46,       d_acc: 0.72, g_loss: 1.86\n",
            "epoch: 17501/30000, d_loss: 0.48,       d_acc: 0.84, g_loss: 2.55\n",
            "epoch: 17601/30000, d_loss: 0.47,       d_acc: 0.80, g_loss: 1.60\n",
            "epoch: 17701/30000, d_loss: 0.41,       d_acc: 0.77, g_loss: 1.89\n",
            "epoch: 17801/30000, d_loss: 0.33,       d_acc: 0.91, g_loss: 1.76\n",
            "epoch: 17901/30000, d_loss: 0.41,       d_acc: 0.80, g_loss: 1.59\n",
            "epoch: 18001/30000, d_loss: 0.42,       d_acc: 0.84, g_loss: 2.08\n",
            "epoch: 18101/30000, d_loss: 0.60,       d_acc: 0.62, g_loss: 1.10\n",
            "epoch: 18201/30000, d_loss: 0.63,       d_acc: 0.64, g_loss: 1.04\n",
            "epoch: 18301/30000, d_loss: 0.49,       d_acc: 0.73, g_loss: 1.10\n",
            "epoch: 18401/30000, d_loss: 0.51,       d_acc: 0.72, g_loss: 1.32\n",
            "epoch: 18501/30000, d_loss: 0.47,       d_acc: 0.80, g_loss: 1.48\n",
            "epoch: 18601/30000, d_loss: 0.50,       d_acc: 0.73, g_loss: 1.70\n",
            "epoch: 18701/30000, d_loss: 0.39,       d_acc: 0.83, g_loss: 1.57\n",
            "epoch: 18801/30000, d_loss: 0.45,       d_acc: 0.80, g_loss: 1.62\n",
            "epoch: 18901/30000, d_loss: 0.64,       d_acc: 0.73, g_loss: 1.35\n",
            "epoch: 19001/30000, d_loss: 0.43,       d_acc: 0.80, g_loss: 1.33\n",
            "epoch: 19101/30000, d_loss: 0.38,       d_acc: 0.83, g_loss: 1.69\n",
            "epoch: 19201/30000, d_loss: 0.35,       d_acc: 0.84, g_loss: 2.17\n",
            "epoch: 19301/30000, d_loss: 0.36,       d_acc: 0.83, g_loss: 1.86\n",
            "epoch: 19401/30000, d_loss: 0.62,       d_acc: 0.72, g_loss: 1.57\n",
            "epoch: 19501/30000, d_loss: 0.50,       d_acc: 0.73, g_loss: 1.48\n",
            "epoch: 19601/30000, d_loss: 0.37,       d_acc: 0.84, g_loss: 1.80\n",
            "epoch: 19701/30000, d_loss: 0.50,       d_acc: 0.77, g_loss: 1.40\n",
            "epoch: 19801/30000, d_loss: 0.51,       d_acc: 0.73, g_loss: 1.21\n",
            "epoch: 19901/30000, d_loss: 0.39,       d_acc: 0.78, g_loss: 2.04\n",
            "epoch: 20001/30000, d_loss: 0.42,       d_acc: 0.78, g_loss: 1.87\n",
            "epoch: 20101/30000, d_loss: 0.48,       d_acc: 0.77, g_loss: 1.51\n",
            "epoch: 20201/30000, d_loss: 0.39,       d_acc: 0.83, g_loss: 1.56\n",
            "epoch: 20301/30000, d_loss: 0.42,       d_acc: 0.81, g_loss: 1.72\n",
            "epoch: 20401/30000, d_loss: 0.33,       d_acc: 0.86, g_loss: 1.81\n",
            "epoch: 20501/30000, d_loss: 0.43,       d_acc: 0.77, g_loss: 1.83\n",
            "epoch: 20601/30000, d_loss: 0.47,       d_acc: 0.75, g_loss: 2.08\n",
            "epoch: 20701/30000, d_loss: 0.43,       d_acc: 0.77, g_loss: 1.93\n",
            "epoch: 20801/30000, d_loss: 0.38,       d_acc: 0.81, g_loss: 2.48\n",
            "epoch: 20901/30000, d_loss: 0.35,       d_acc: 0.88, g_loss: 1.67\n",
            "epoch: 21001/30000, d_loss: 0.39,       d_acc: 0.84, g_loss: 1.67\n",
            "epoch: 21101/30000, d_loss: 0.56,       d_acc: 0.67, g_loss: 2.36\n",
            "epoch: 21201/30000, d_loss: 0.31,       d_acc: 0.91, g_loss: 2.05\n",
            "epoch: 21301/30000, d_loss: 1.63,       d_acc: 0.55, g_loss: 1.80\n",
            "epoch: 21401/30000, d_loss: 1.06,       d_acc: 0.47, g_loss: 1.06\n",
            "epoch: 21501/30000, d_loss: 0.63,       d_acc: 0.62, g_loss: 1.28\n",
            "epoch: 21601/30000, d_loss: 0.91,       d_acc: 0.48, g_loss: 1.14\n",
            "epoch: 21701/30000, d_loss: 0.76,       d_acc: 0.45, g_loss: 1.11\n",
            "epoch: 21801/30000, d_loss: 0.59,       d_acc: 0.67, g_loss: 1.01\n",
            "epoch: 21901/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 1.11\n",
            "epoch: 22001/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.88\n",
            "epoch: 22101/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.83\n",
            "epoch: 22201/30000, d_loss: 0.67,       d_acc: 0.53, g_loss: 0.83\n",
            "epoch: 22301/30000, d_loss: 0.70,       d_acc: 0.62, g_loss: 1.15\n",
            "epoch: 22401/30000, d_loss: 0.78,       d_acc: 0.50, g_loss: 0.81\n",
            "epoch: 22501/30000, d_loss: 0.72,       d_acc: 0.55, g_loss: 1.09\n",
            "epoch: 22601/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 1.03\n",
            "epoch: 22701/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.98\n",
            "epoch: 22801/30000, d_loss: 0.80,       d_acc: 0.47, g_loss: 1.12\n",
            "epoch: 22901/30000, d_loss: 0.67,       d_acc: 0.52, g_loss: 1.01\n",
            "epoch: 23001/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 1.05\n",
            "epoch: 23101/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.90\n",
            "epoch: 23201/30000, d_loss: 0.52,       d_acc: 0.75, g_loss: 1.32\n",
            "epoch: 23301/30000, d_loss: 0.57,       d_acc: 0.70, g_loss: 1.08\n",
            "epoch: 23401/30000, d_loss: 0.50,       d_acc: 0.81, g_loss: 1.31\n",
            "epoch: 23501/30000, d_loss: 0.49,       d_acc: 0.77, g_loss: 1.11\n",
            "epoch: 23601/30000, d_loss: 0.51,       d_acc: 0.81, g_loss: 1.57\n",
            "epoch: 23701/30000, d_loss: 0.49,       d_acc: 0.77, g_loss: 1.53\n",
            "epoch: 23801/30000, d_loss: 0.52,       d_acc: 0.72, g_loss: 1.55\n",
            "epoch: 23901/30000, d_loss: 0.46,       d_acc: 0.81, g_loss: 1.35\n",
            "epoch: 24001/30000, d_loss: 0.43,       d_acc: 0.78, g_loss: 1.62\n",
            "epoch: 24101/30000, d_loss: 0.51,       d_acc: 0.75, g_loss: 1.66\n",
            "epoch: 24201/30000, d_loss: 0.52,       d_acc: 0.77, g_loss: 1.33\n",
            "epoch: 24301/30000, d_loss: 0.34,       d_acc: 0.91, g_loss: 1.64\n",
            "epoch: 24401/30000, d_loss: 0.48,       d_acc: 0.83, g_loss: 1.95\n",
            "epoch: 24501/30000, d_loss: 0.50,       d_acc: 0.78, g_loss: 1.83\n",
            "epoch: 24601/30000, d_loss: 0.42,       d_acc: 0.83, g_loss: 1.50\n",
            "epoch: 24701/30000, d_loss: 0.50,       d_acc: 0.77, g_loss: 1.39\n",
            "epoch: 24801/30000, d_loss: 0.45,       d_acc: 0.81, g_loss: 1.67\n",
            "epoch: 24901/30000, d_loss: 0.45,       d_acc: 0.73, g_loss: 1.81\n",
            "epoch: 25001/30000, d_loss: 0.48,       d_acc: 0.78, g_loss: 1.71\n",
            "epoch: 25101/30000, d_loss: 0.51,       d_acc: 0.77, g_loss: 1.63\n",
            "epoch: 25201/30000, d_loss: 0.61,       d_acc: 0.66, g_loss: 1.79\n",
            "epoch: 25301/30000, d_loss: 0.39,       d_acc: 0.78, g_loss: 1.69\n",
            "epoch: 25401/30000, d_loss: 0.54,       d_acc: 0.75, g_loss: 1.52\n",
            "epoch: 25501/30000, d_loss: 0.71,       d_acc: 0.59, g_loss: 1.59\n",
            "epoch: 25601/30000, d_loss: 0.44,       d_acc: 0.78, g_loss: 1.84\n",
            "epoch: 25701/30000, d_loss: 0.58,       d_acc: 0.72, g_loss: 1.63\n",
            "epoch: 25801/30000, d_loss: 0.51,       d_acc: 0.73, g_loss: 1.97\n",
            "epoch: 25901/30000, d_loss: 0.36,       d_acc: 0.81, g_loss: 2.00\n",
            "epoch: 26001/30000, d_loss: 0.55,       d_acc: 0.70, g_loss: 1.41\n",
            "epoch: 26101/30000, d_loss: 0.54,       d_acc: 0.73, g_loss: 1.47\n",
            "epoch: 26201/30000, d_loss: 0.29,       d_acc: 0.89, g_loss: 1.68\n",
            "epoch: 26301/30000, d_loss: 0.59,       d_acc: 0.66, g_loss: 1.48\n",
            "epoch: 26401/30000, d_loss: 0.38,       d_acc: 0.88, g_loss: 1.70\n",
            "epoch: 26501/30000, d_loss: 0.47,       d_acc: 0.72, g_loss: 1.78\n",
            "epoch: 26601/30000, d_loss: 0.42,       d_acc: 0.83, g_loss: 1.91\n",
            "epoch: 26701/30000, d_loss: 0.54,       d_acc: 0.77, g_loss: 2.06\n",
            "epoch: 26801/30000, d_loss: 0.52,       d_acc: 0.84, g_loss: 2.16\n",
            "epoch: 26901/30000, d_loss: 0.63,       d_acc: 0.61, g_loss: 1.46\n",
            "epoch: 27001/30000, d_loss: 0.40,       d_acc: 0.81, g_loss: 1.70\n",
            "epoch: 27101/30000, d_loss: 0.43,       d_acc: 0.77, g_loss: 1.94\n",
            "epoch: 27201/30000, d_loss: 0.44,       d_acc: 0.80, g_loss: 2.01\n",
            "epoch: 27301/30000, d_loss: 0.60,       d_acc: 0.69, g_loss: 1.73\n",
            "epoch: 27401/30000, d_loss: 0.46,       d_acc: 0.78, g_loss: 1.69\n",
            "epoch: 27501/30000, d_loss: 0.45,       d_acc: 0.73, g_loss: 1.53\n",
            "epoch: 27601/30000, d_loss: 0.40,       d_acc: 0.80, g_loss: 1.90\n",
            "epoch: 27701/30000, d_loss: 0.53,       d_acc: 0.72, g_loss: 2.00\n",
            "epoch: 27801/30000, d_loss: 0.56,       d_acc: 0.73, g_loss: 1.72\n",
            "epoch: 27901/30000, d_loss: 0.50,       d_acc: 0.75, g_loss: 1.64\n",
            "epoch: 28001/30000, d_loss: 0.46,       d_acc: 0.75, g_loss: 2.12\n",
            "epoch: 28101/30000, d_loss: 0.37,       d_acc: 0.83, g_loss: 1.92\n",
            "epoch: 28201/30000, d_loss: 0.54,       d_acc: 0.72, g_loss: 2.34\n",
            "epoch: 28301/30000, d_loss: 0.44,       d_acc: 0.73, g_loss: 2.04\n",
            "epoch: 28401/30000, d_loss: 0.38,       d_acc: 0.84, g_loss: 2.07\n",
            "epoch: 28501/30000, d_loss: 0.45,       d_acc: 0.80, g_loss: 1.66\n",
            "epoch: 28601/30000, d_loss: 0.40,       d_acc: 0.83, g_loss: 1.64\n",
            "epoch: 28701/30000, d_loss: 0.44,       d_acc: 0.80, g_loss: 1.98\n",
            "epoch: 28801/30000, d_loss: 0.35,       d_acc: 0.92, g_loss: 1.88\n",
            "epoch: 28901/30000, d_loss: 0.35,       d_acc: 0.88, g_loss: 1.57\n",
            "epoch: 29001/30000, d_loss: 0.32,       d_acc: 0.89, g_loss: 1.85\n",
            "epoch: 29101/30000, d_loss: 0.35,       d_acc: 0.83, g_loss: 1.91\n",
            "epoch: 29201/30000, d_loss: 0.43,       d_acc: 0.80, g_loss: 1.76\n",
            "epoch: 29301/30000, d_loss: 0.51,       d_acc: 0.77, g_loss: 1.66\n",
            "epoch: 29401/30000, d_loss: 0.46,       d_acc: 0.78, g_loss: 1.94\n",
            "epoch: 29501/30000, d_loss: 0.32,       d_acc: 0.88, g_loss: 1.89\n",
            "epoch: 29601/30000, d_loss: 0.50,       d_acc: 0.77, g_loss: 1.83\n",
            "epoch: 29701/30000, d_loss: 0.50,       d_acc: 0.72, g_loss: 1.56\n",
            "epoch: 29801/30000, d_loss: 0.45,       d_acc: 0.73, g_loss: 2.01\n",
            "epoch: 29901/30000, d_loss: 0.36,       d_acc: 0.83, g_loss: 2.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-WfLHXs9MW",
        "colab_type": "code",
        "outputId": "0f7d3771-138e-46d4-d4eb-25a770a38159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(g_losses, label='g_losses')\n",
        "plt.plot(d_losses, label='d_losses')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f99a8436ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKLV-8RKzPvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUTvx6WrzT51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}